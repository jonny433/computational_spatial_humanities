{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: -Duplicatebereinigung\n",
    "-Date\n",
    "-CSV-Append realisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "soup_size_price = pd.DataFrame()\n",
    "soup_address = pd.DataFrame()\n",
    "temp_price_size= pd.DataFrame()\n",
    "temp_address= pd.DataFrame()\n",
    "final_frame= pd.DataFrame()\n",
    "final_df = pd.DataFrame()\n",
    "summary_df = pd.DataFrame()\n",
    "sleep_sec=12\n",
    "#30\n",
    "for i in range(20,22):\n",
    "   \n",
    "   print('Round 1:'+str(i))\n",
    "   #https://www.wg-gesucht.de/wg-zimmer-und-1-zimmer-wohnungen-und-wohnungen-in-Leipzig.77.0+1+2.1.0.html?offer_filter=1&city_id=77&sort_column=6&sort_order=0&noDeact=1&categories%5B%5D=0&categories%5B%5D=1&categories%5B%5D=2&rent_types%5B%5D=2&rent_types%5B%5D=2\n",
    "   Url = 'https://www.wg-gesucht.de/wg-zimmer-und-1-zimmer-wohnungen-und-wohnungen-in-Leipzig.77.0+1+2.1.'+str(i)+'.html'\n",
    "   print(Url)\n",
    "\n",
    "\n",
    "   soup = get_soup(Url)\n",
    "   #print(soup)\n",
    "\n",
    "   #returns a df with the link of each flat\n",
    "   link=get_subpages_from_soup(soup)\n",
    "   #returns all information of the page lists\n",
    "   front_page=get_front_page_data(soup)\n",
    "   #concats list url with fronpage data\n",
    "   front_frame= concat_link_address(link,front_page)\n",
    "   #return all data from each subpage\n",
    "   back_frame = get_sub_page_alpha(soup)\n",
    "   #connects the front and backframe via the key url and link\n",
    "   final_df= back_frame.merge(front_frame, left_on='Url', right_on='link')\n",
    "\n",
    "   summary_df = pd.concat([summary_df,final_df])\n",
    "   #waiting time between pages\n",
    "   time.sleep(randint(1*100, sleep_sec*100)/100)\n",
    "   #final_df= pd.concat([front_frame,get_sub_page_alpha(soup)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import traceback\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test stuff checking each variable\n",
    "#----------------------------------\n",
    "#display(soup_size_price)\n",
    "#display(soup_address)\n",
    "#display(temp_price_size)\n",
    "#display(temp_address)\n",
    "#display(final_frame)\n",
    "#display(back_frame)\n",
    "#display(final_df)\n",
    "#len(summary_df)\n",
    "display(summary_df)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(summary_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safe Data in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only relevant for the first time \n",
    "summary_df.to_csv('Summary_subpages19.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append Dataframe to existing CSV\n",
    "summary_df.to_csv('Summary_subpages2.csv', mode='a',index=False, header=False)\n",
    "\n",
    "print(\"DataFrame wurde erfolgreich an die CSV-Datei angehängt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickeled = pd.read_csv('Summary_subpages2.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicate_free = pd.read_csv('Summary_subpages2.csv',delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=df_duplicate_free.drop_duplicates(subset=['Url'], keep='first', inplace=False, ignore_index=False)\n",
    "new_dup_free=new.dropna(subset=['Street', 'Address','rent_cold','additional_cost'])\n",
    "len(new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dup_free.to_csv('Summary_subpages_duplicate_free.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new['Street'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pickeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(df_pickeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_homes = pd.read_csv(\"Summary_subpages2.csv\")\n",
    "df_homes1 = pd.read_csv(\"C:/Users/Oliver/Desktop/Unizeug/Master/2.Semester/Spatial/Summary_subpages.csv\")\n",
    "pd.concat([df_homes, df_homes1]).to_csv('homes_complete.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD Map data\n",
    "#Additional task cos nominatim service fails sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add long lat Data to geo_df\n",
    "#print´s address if legit address available \n",
    "geo_df= get_geo_location(df_pickeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df= get_geo_location(new_dup_free)\n",
    "#new_dup_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(geo_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second CSV for safety reasons^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only relevant for the first time  creates CSV File\n",
    "#\n",
    "geo_df.to_csv('Summary_subpages_w_location2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_pickeled = pd.read_csv('Summary_subpages_w_location2.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df_pickeled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################################################################\n",
    "Functionsblock\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geo_location(df_wg_ges):\n",
    "    import geopy\n",
    "    user_agent = 'user_me_{}'.format(randint(10000,99999))\n",
    "    geolocator = Nominatim(user_agent=user_agent)\n",
    "    sleep_sec=1\n",
    "    \n",
    "    #parsing problem troubleshooting, otherwise pyhton creates string with float value...\n",
    "    df_wg_ges['location'] = 0.0\n",
    "    df_wg_ges['latitude'] = 0.0\n",
    "    df_wg_ges['longitude'] = 0.0\n",
    "\n",
    "    for inx in range(len(df_wg_ges)):\n",
    "        print(inx)\n",
    "        if(len(str(df_wg_ges['Street'].iat[inx]))==0 and len(str(df_wg_ges['Address'].iat[inx])) ==0):\n",
    "            continue\n",
    "        temp=str(df_wg_ges['Street'].iat[inx] +' '+ df_wg_ges['Address'].iat[inx])\n",
    "        time.sleep(randint(1*100, sleep_sec*100)/100)\n",
    "        location = geolocator.geocode(temp)\n",
    "        print(location)\n",
    "        if(location==None):\n",
    "            continue\n",
    "        df_wg_ges['location'].iat[inx] = location\n",
    "        df_wg_ges['latitude'].iat[inx] = location.latitude\n",
    "        df_wg_ges['longitude'].iat[inx] = location.longitude\n",
    "    \n",
    "    return df_wg_ges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_link_address(link,temp_address):\n",
    "     temp_address['link']=link\n",
    "     return temp_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Soup from mainpage # I#m using headful browsing for avoiding bot-blocks\n",
    "\n",
    "def get_soup(url):\n",
    "    \n",
    "    print('current URL: '+str(url))\n",
    "    #url=\"https://www.wg-gesucht.de/1-zimmer-wohnungen-in-Leipzig.77.1.1.0.html\"\n",
    "\n",
    "    result = requests.get(url,headers = {'User-Agent':'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(result.text, 'html.parser') ##lxml as parser\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from requests_tor import RequestsTor\n",
    "def get_soup(url):\n",
    "    rt = RequestsTor(tor_ports=(9050,), tor_cport=9051)\n",
    "\n",
    "    #url=\"https://www.wg-gesucht.de/1-zimmer-wohnungen-in-Leipzig.77.1.1.0.html\"\n",
    "    params = {\n",
    "        \"id\": 12345,\n",
    "        \"status\": 'passed'\n",
    "        }\n",
    "    headers = {\n",
    "        \"Origin\": \"https://www.wg-gesucht.de\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\"\n",
    "        }\n",
    "    result = rt.get(url, params=params, headers=headers)\n",
    "    #print(r.text)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    return soup\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_page_alpha(soup):\n",
    "    \n",
    "    flats = soup.find_all(\n",
    "            class_=\"truncate_title noprint\"\n",
    "        )\n",
    "    \n",
    "    sleep_sec=1\n",
    "    #place holder lists for every feature\n",
    "    list_accessible = []\n",
    "    list_rent_cold = []\n",
    "    list_additional_cost = []\n",
    "    list_distance = []\n",
    "    list_gadgets = []\n",
    "    list_female_roommates = []\n",
    "    list_male_roommates = []\n",
    "    list_divers_roommates = []\n",
    "    list_room_description = []\n",
    "    list_location_description = []\n",
    "    list_flat_live = []\n",
    "    list_additional = []\n",
    "    list_smoker =[]\n",
    "    list_kind_of_flat = []\n",
    "    list_url = []\n",
    "    list_free_from = []\n",
    "    list_plz_city= []\n",
    "    list_street = []\n",
    "\n",
    "   \n",
    "    for idx,flat in enumerate (flats):\n",
    "            \n",
    "            time.sleep(randint(1*100, sleep_sec*100)/100)\n",
    "            #navigate to subpage and create subpage_Url\n",
    "            flat_url = str('https://www.wg-gesucht.de')+str(flat.find('a')['href'])\n",
    "            if 'asset_id' in flat_url:\n",
    "                continue\n",
    "            #flat_url = str('https://www.wg-gesucht.de/wg-zimmer-in-Leipzig-Volkmarsdorf.5200825.html')\n",
    "            #create soup of subpage\n",
    "            flat_soup = get_soup(flat_url)\n",
    "\n",
    "            #invoke scraped text funktions\n",
    "            #using BS4 funtions to shape the soup\n",
    "            try:\n",
    "                rent_cold_additional_cost = flat_soup.find(class_= 'col-sm-5').text.replace('\\n', '').replace('   ', '')\n",
    "            except:\n",
    "                rent_cold_additional_cost = ''\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                if('Barrierefrei'== flat_soup.find(class_= 'mdi mdi-wheelchair-accessibility mdi-36px noprint').parent.text.replace('\\n', '').replace(' ', '') ):\n",
    "                    accessible = 1\n",
    "            except:\n",
    "                accessible = 0\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                distance = flat_soup.find(class_= 'mdi mdi-bus mdi-36px noprint').parent.text.replace('\\n', '').replace('   ', '')\n",
    "            except:\n",
    "                distance = ''\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                gadgets = flat_soup.find(class_= 'mdi mdi-folder mdi-36px noprint').parent.text.replace('\\n', '').replace(' ', '')\n",
    "            except:\n",
    "                gadgets = ''\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                roommates = flat_soup.find(class_= 'col-sm-6').text.replace('\\n', '').replace('   ', '')\n",
    "\n",
    "            except:\n",
    "                roommates= ''\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                room_description = flat_soup.find(id= 'freitext_0_content').text.replace('\\n', '').replace('   ', '')\n",
    "            except:\n",
    "                room_description= np.NaN\n",
    "                pass\n",
    "            try:\n",
    "                location_description = flat_soup.find(id= 'freitext_1_content').text.replace('\\n', '').replace('   ', '')\n",
    "            except:\n",
    "                location_description = np.NaN\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                flat_live = flat_soup.find(id= 'freitext_2_content').text.replace('\\n', '').replace('  ', '')\n",
    "            except:\n",
    "                flat_live= np.NaN\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                additional = flat_soup.find(id= 'freitext_3_content').text.replace('\\n', '').replace('   ', '')\n",
    "            except:\n",
    "                additional= np.NaN\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                free_from = flat_soup.find(class_= 'col-sm-3').text.replace('\\n', '').replace('   ', '')\n",
    "            except:\n",
    "                free_from = ''\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                address = flat_soup.find(class_= 'col-sm-4 mb10').text.replace('\\n\\n', '').replace('   ', '').replace('Umzugsfirma beauftragen1', '').replace('Adresse', '')\n",
    "            except:\n",
    "                address = ''\n",
    "                pass\n",
    "            \n",
    "            #Append the modified scraped text blocks to list\n",
    "\n",
    "            #Barrierefrei\n",
    "            list_accessible.append(accessible)\n",
    "            #Kaltmiete\n",
    "            list_rent_cold.append(prepare_rent_cold(rent_cold_additional_cost))\n",
    "            #Nebenkosten\n",
    "            list_additional_cost.append(prepare_additional_cost(rent_cold_additional_cost))\n",
    "            #Distanze zu Öffis\n",
    "            list_distance.append(prepare_distance(distance))\n",
    "            #Ausstattung der Wohnung\n",
    "            list_gadgets.append(gadgets)\n",
    "            #Weibliche Mitbewohnerinnen\n",
    "            list_female_roommates.append(prepare_female_roommates(roommates))\n",
    "            #männliche Mitbewohner\n",
    "            list_male_roommates.append(prepare_male_roommates(roommates))\n",
    "            #diverse Mitbewohnende\n",
    "            list_divers_roommates.append(prepare_divers_roommates(roommates))\n",
    "            #Freitext Zimmerbeschreibung\n",
    "            list_room_description.append(room_description)\n",
    "            #Freitext Lagebeschreibung\n",
    "            list_location_description.append(location_description)\n",
    "            #Freitext WgLeben\n",
    "            list_flat_live.append(flat_live)\n",
    "            #Freitext Generellen Infos\n",
    "            list_additional.append(additional)\n",
    "            #RaucherWg oder nicht (Alle sind raucher außer explizite Nichtraucher WGs)\n",
    "            list_smoker.append(prepare_smoker(roommates))\n",
    "            #Art des Zusammenlebens\n",
    "            list_kind_of_flat.append(prepare_kind_of_flat(roommates))\n",
    "            #Add flat url (unique key)\n",
    "            list_url.append(flat_url)\n",
    "            #Einzugsdatum\n",
    "            list_free_from.append(get_date_from_text(free_from))\n",
    "            #Postalcode and City name\n",
    "            list_plz_city.append(get_plz_city(address))\n",
    "            #Streetaddress\n",
    "            list_street.append(get_street(address))\n",
    "    #Add everything to Dataframe at once\n",
    "    d = {'Street': list_street,'Address': list_plz_city,'rent_cold':list_rent_cold,'additional_cost':list_additional_cost,'free_from': list_free_from, 'distance': list_distance, 'gadgets': list_gadgets,'female_roommates':list_female_roommates,'male_roommates': list_male_roommates,'divers_roommates':list_divers_roommates,'room_description':list_room_description,'location_description': list_location_description,'flat_live':list_flat_live,'additional':list_additional,'smoker':list_smoker,'accessible': list_accessible,'kind_of_flat':list_kind_of_flat,'Url':list_url}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the city and the postalcode as one string\n",
    "def get_plz_city(text):\n",
    "\n",
    "    pattern = r'\\d{5}.*?(?=\\n)'\n",
    "\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if(matches):\n",
    "        for match in matches:\n",
    "            \n",
    "            return match\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_street(text):\n",
    "    #Seach for everything between the first two \\n\n",
    "    pattern = r'(?<=\\n).*?(?=\\n)'\n",
    "\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return (match.group(0))\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_text(text):\n",
    "    if not text:\n",
    "        return np.NaN\n",
    "\n",
    "    pattern = r\"\\b(\\d{2}\\.\\d{2}\\.\\d{4})\\b\"\n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    \n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        matched_date = match.group(1)\n",
    "        return matched_date   \n",
    "    else: \n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_distance(text):\n",
    "    if not text:\n",
    "        return np.NaN\n",
    "    return text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rent_cold(text):\n",
    "    if not text:\n",
    "        return np.NaN\n",
    "    pattern = r\":(.*?)€\"\n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    if match:\n",
    "        captured_text = match.group(1)\n",
    "        return captured_text\n",
    "    else:\n",
    "        return np.NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_additional_cost(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    pattern = r\"\\.\\s\\s(.*?)€\"\n",
    "\n",
    "    match = re.search(pattern, text)\n",
    "    if match :\n",
    "        captured_text = match.group(1)\n",
    "        if(len(captured_text) <= 4):\n",
    "            try:\n",
    "                if captured_text:\n",
    "                    return(captured_text)\n",
    "            except:\n",
    "                return 0  \n",
    "        else:\n",
    "            return 0 \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_divers_roommates(text):\n",
    "    \n",
    "    if not text:\n",
    "        return np.NaN\n",
    "    \n",
    "    pattern_divers = r\"(\\d+)\\s+Divers\"\n",
    "    \n",
    "    #singular & plural\n",
    "    match_divers = re.search(pattern_divers, text)\n",
    "    \n",
    "    if match_divers:\n",
    "        captured_number = match_divers.group(1)\n",
    "        return captured_number\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_female_roommates(text):\n",
    "    if not text:\n",
    "        return np.NaN\n",
    "    #text = \"The number 3 Frau is mentioned here.\"\n",
    "    pattern_woman = r\"(\\d+)\\s+Frau\"\n",
    "    pattern_women = r\"(\\d+)\\s+Frauen\"\n",
    "    \n",
    "    \n",
    "    #singular\n",
    "    match_woman = re.search(pattern_woman, text)\n",
    "    #plural\n",
    "    match_women = re.search(pattern_women, text)\n",
    "\n",
    "    if match_woman:\n",
    "        captured_number = match_woman.group(1)\n",
    "        return captured_number\n",
    "    elif match_women:\n",
    "        captured_number = match_women.group(1)\n",
    "        return captured_number\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_male_roommates(text):\n",
    "    if not text:\n",
    "        return np.NaN\n",
    "    \n",
    "    #text = \"The number 3 Frau is mentioned here.\"\n",
    "    \n",
    "    pattern_man = r\"(\\d+)\\s+Mann\"\n",
    "    pattern_men = r\"(\\d+)\\s+Männer\"\n",
    "    \n",
    "    #singular\n",
    "    match_man = re.search(pattern_man, text)\n",
    "    #plural\n",
    "    match_men = re.search(pattern_men, text)\n",
    "\n",
    "    if match_man:\n",
    "        captured_number = match_man.group(1)\n",
    "        return captured_number\n",
    "    elif match_men:\n",
    "        captured_number = match_men.group(1)\n",
    "        return captured_number\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_smoker(text):\n",
    "    if not text:\n",
    "        return np.NaN\n",
    "    \n",
    "    pattern1 = r\"Rauchen nicht erwünscht\"\n",
    "    pattern2 = r\"Rauchen auf dem Balkon erlaubt\"\n",
    "    pattern3 = r\"Rauchen im Zimmer erlaubt\"\n",
    "    pattern4 = r\"Rauchen überall erlaubt\"\n",
    "\n",
    "    matches1 = re.search(pattern1, text)\n",
    "    matches2 = re.search(pattern2, text)\n",
    "    matches3 = re.search(pattern3, text)\n",
    "    matches4 = re.search(pattern4, text)\n",
    "    \n",
    "    if matches1:\n",
    "        return int(0)\n",
    "    elif matches2:\n",
    "        return int(1)\n",
    "    elif matches3:\n",
    "        return int(1)\n",
    "    elif matches4:\n",
    "        return int(1)\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_kind_of_flat(text):\n",
    "    if not text:\n",
    "        return np.NaN\n",
    "    \n",
    "    options = [\"gemischte WG\", \"Studenten-WG\", \"WG-Neugründung\", \"Verbindung\", \"Alleinerziehende\", \"Senioren-WG\",\n",
    "            \"Wohnen für Hilfe\", \"funktionale WG\", \"inklusive WG\", \"Plus-WG\", \"Azubi-WG\", \"Wohnheim\",\n",
    "            \"Männer-WG\", \"Mehrgenerationen\", \"Vegetarisch/Vegan\", \"Business-WG\", \"keine Zweck-WG\", \"WG mit Kindern\",\n",
    "            \"LGBTQIA+\", \"Internationals welcome\", \"Frauen-WG\", \"Zweck-WG\"]\n",
    "\n",
    "    # Create the regular expression pattern by joining the phrases with the \"|\" (OR) operator\n",
    "    pattern = r\"\\b(\" + \"|\".join(map(re.escape, options)) + r\")\\b\"\n",
    "\n",
    "    # Find all occurrences of the phrases in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    matches_as_string= str()\n",
    "    \"\"\"\n",
    "    for i in matches:\n",
    "        matches_as_string= matches_as_string+str(matches)\n",
    "    # return the matches\n",
    "    \"\"\"\n",
    "    return(str(matches).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price(text):\n",
    "    match = re.search(r'(.*?)€', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "        \n",
    "    else:\n",
    "        return np.NaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retuns the flat/room size\n",
    "def get_size(text):\n",
    "    match = re.search(r'€(.*)', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "        \n",
    "    else:\n",
    "        return np.NaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delets the datevalue out of the text\n",
    "def replace_date_in_string(text):\n",
    "    return re.sub(r'\\d{2}\\.\\d{2}\\.\\d{4}', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_front_page_data(soup):\n",
    "    flats = soup.find_all(\n",
    "                class_=\"col-sm-8 card_body\"\n",
    "            )\n",
    "    #create empty lists\n",
    "    art=[]\n",
    "    district=[]\n",
    "    size=[]\n",
    "    price=[]\n",
    "    df = pd.DataFrame()\n",
    "    for idx, x in enumerate(flats):\n",
    "        #some flats are offert by companies and if you click on the flat you get a list insteat of the normal view\n",
    "        if 'Verifiziertes Unternehmen'  in str(x.find(class_= 'row noprint bottom').text.replace('\\n','').replace('   ','')):\n",
    "            continue\n",
    "        else:\n",
    "            art.append(get_type(str(x.find(class_= 'col-xs-11').text.replace('\\n','').replace('   ',''))))\n",
    "            district.append(get_district(str(x.find(class_= 'col-xs-11').text.replace('\\n','').replace(' ','').replace('Leipzig',''))))\n",
    "            text =replace_date_in_string(x.find(class_= 'row noprint middle').text.replace('\\n','').replace('-','').replace(' m²','').replace('   ',''))\n",
    "            size.append(get_size(text))\n",
    "            price.append(get_price(text))\n",
    "\n",
    "    df['type']=art\n",
    "    df['district']= district  \n",
    "    df['size']=size\n",
    "    df['price']=price\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the type of a flat/room( 1-zimmer wohung, 3er-Wg...)\n",
    "def get_type(text):\n",
    "    match = re.search(r'^(.*?)\\|', text)\n",
    "    if match:\n",
    "        result = match.group(1)\n",
    "        return result\n",
    "    else:\n",
    "        return np.NaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the district of the flat\n",
    "def get_district(text):\n",
    "    match = re.search(r'\\|([^|]*)\\|', text)\n",
    "    if match:\n",
    "        return match.group(1)   \n",
    "    else:\n",
    "        return np.NaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#navigates the soup of the frontpage to the soup of every flat\n",
    "def get_subpages_from_soup(soup):\n",
    "    \n",
    "    subpage_list=[]\n",
    "   \n",
    "    flats = soup.find_all(\n",
    "            class_=\"truncate_title noprint\"\n",
    "        )\n",
    "    \n",
    "    for idx,flat in enumerate (flats):\n",
    "            \n",
    "            #time.sleep(randint(1*100, sleep_sec*100)/100)\n",
    "            #navigate to subpage and create subpage_Url\n",
    "            temp = str('https://www.wg-gesucht.de')+str(flat.find('a')['href'])\n",
    "            if 'asset_id' not in temp:\n",
    "                subpage_list.append(temp)\n",
    "            else:\n",
    "                 continue\n",
    "   \n",
    "    return subpage_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
