{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = \"../data/\"\n",
    "keplergl_config_path = \"../keplergl_config/\"\n",
    "os.listdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(f\"{data_path}wg_gesucht.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{data_path}Summary_subpages_w_location_Backup_06_06.txt\", sep=',')\n",
    "df = df.convert_dtypes()\n",
    "for column in [\"free_from\"]:\n",
    "    df[column] = df[column].astype(str).str.zfill(8)  # Füge führende Nullen hinzu, falls erforderlich\n",
    "    df[column] = df[column] + f\" 00:00:{np.random.randint(1, 59)}\"\n",
    "    df[column] = pd.to_datetime(df[column], format=\"%d%m%Y %H:%M:%S\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine leere Spalte mit dem Namen \"full_description\"\n",
    "df['full_description'] = np.nan\n",
    "\n",
    "# Kombiniere die Werte der ausgewählten Spalten zu einer neuen Spalte \"full_description\"\n",
    "df['full_description'] = df[['room_description', 'location_description', 'flat_live', 'additional']].apply(\n",
    "    lambda x: ' '.join([str(i) for i in x if not pd.isna(i)]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Places, Persons, Organisationen hinzufühgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der verfügbaren Labels\n",
    "labels = nlp.get_pipe(\"ner\").labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text: str)-> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Extrahiert Orte, Personen und Organisationen aus dem gegebenen Text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Der Text, aus dem die Entitäten extrahiert werden sollen.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str], List[str]]: Ein Tupel, das die extrahierten Entitäten enthält.\n",
    "            Das erste Element ist eine Liste von Orten, das zweite Element eine Liste von Personen\n",
    "            und das dritte Element eine Liste von Organisationen.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    places = [token.text for token in doc.ents if token.label_ == \"LOC\"]\n",
    "    persons = [token.text for token in doc.ents if token.label_ == \"PER\"]\n",
    "    organizations = [token.text for token in doc.ents if token.label_ == \"ORG\"]\n",
    "    return places, persons, organizations\n",
    "\n",
    "# Anwenden der Spacy-Pipeline auf die Spalte \"full_description\"\n",
    "df[[\"places\", \"persons\", \"organizations\"]] = df[\"full_description\"].apply(extract_entities).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places mit Koordinaten versehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_places = set()\n",
    "\n",
    "for places_list in df['places']:\n",
    "    unique_places.update(places_list)\n",
    "print(len(unique_places), unique_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from time import sleep\n",
    "\n",
    "# Erzeuge einen Nominatim-Geocoder\n",
    "geolocator = Nominatim(user_agent='my_app')\n",
    "\n",
    "# Funktion zum Geokodieren mit Rückversuchen bei Zeitüberschreitung\n",
    "def geocode_with_retry(location):\n",
    "    # city = \"Leipzig\"\n",
    "    # country = \"Deutschland\"\n",
    "    location = f\"{location}\"\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            return geolocator.geocode(location)\n",
    "        except GeocoderTimedOut:\n",
    "            print(f\"Timeout{retry_count}: {location}\")\n",
    "            sleep(2 + retry_count)\n",
    "            retry_count += 1\n",
    "        except:\n",
    "            print(f\"Fehler{retry_count}: {location}\")\n",
    "            break\n",
    "    return None\n",
    "\n",
    "\n",
    "places_dict = {}\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "# for location in unique_places:\n",
    "\n",
    "#     location_data = geocode_with_retry(location)\n",
    "\n",
    "#     if location_data:\n",
    "#         places_dict[location] = {\n",
    "#             \"status\": True,\n",
    "#             \"lon\": location_data.longitude,\n",
    "#             \"lat\": location_data.latitude,\n",
    "#             \"raw\": location_data.raw\n",
    "#         }\n",
    "#         count1 += 1\n",
    "#     else:\n",
    "#         places_dict[location] = {\n",
    "#             \"status\": False,\n",
    "#             \"lon\": 0.0,\n",
    "#             \"lat\": 0.0,\n",
    "#             \"raw\": {}\n",
    "#         }\n",
    "#         count2 += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{data_path}places.json\", \"r\", encoding='utf-8') as file:\n",
    "    places_lpz = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_places = {}\n",
    "for key, value in places_lpz.items():\n",
    "    if value[\"stauts\"] == True:\n",
    "        clean_places[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in clean_places.items():\n",
    "    if value['lat'] > 52 or value['lat'] < 50:\n",
    "        print(f\"{key}: {value['lat']}, {value['lon']}\")\n",
    "    elif value['lon'] > 13 or value['lon'] < 11:\n",
    "        print(f\"{key}: {value['lat']}, {value['lon']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\n",
    "    \"Hauptbahnhof\": [\"Leipziger Hbf.\", \"Hbf.\", \"CENTRALSTATION\", \"Hauptbahhof\"],\n",
    "    # \"öpnv\": [\"Linie 90\", \"straßenbahn/bus\", \"linie\", \"tram\", \"bus\", \"bahn\", \"nahverkehr\", \"nahverkehrstechnisch\", \"S3\", \"Straßenbahn 14\", \"Straßenbahn Linie 14\", \"Straßenbahn 1\", \"Öffis\", \"S-Bahn/Straßenbahn\", \"Straßenbahn Linie 16\"],\n",
    "    \"Völkerschlachtdenkmal\": [\"Völki\"],\n",
    "    # \"staete\": [\"Mainz\"],\n",
    "    \"Clara-Zetkin-Park\": [\"Clarapark\", \"Clara Park\", \"Clara-Zetkin\\\" city park\", \"Clara-Zetkin-Park befindetsich\"],\n",
    "    \"Markkleeberg See\": [],\n",
    "    \"Cospudener See\": [\"Cossi\"],\n",
    "    \"Rosental\": [],\n",
    "    \"Eisenbahnstraße\": [\"Eisi\" ],\n",
    "    \"Wilhelm-Leuschner Platz\": [\"Wilhelm-Leuschner-Platz läufst du\", \"Wilhelm-Leuchner\"],\n",
    "    \"Jahnallee\": [\"Jahnallee-Kampus\", \"Sport-Uni\", \"RedBull Arena\"],\n",
    "    \"universität leipzig\": [\"Uni Hauptgebäude\", \"MDR Turm\", \"Mensa\"],\n",
    "    \"Kulkwitzer\": [],\n",
    "    \"Lene-Voigt-Park\": [\"Lenepark\"],\n",
    "    \"universität leipzig\": [\"Zoo für Kulturprogramm\", \"Leipziger Zoo\"],\n",
    "    \"Bayrischer Bahnhof\": [\"DerBayrische Bahnhof\", \"Bayrische Bahnhof\"],\n",
    "    # \"Lebensmittel\": [\"Penny\", \"Norma\", \"Netto\", \"Rossman\", \"Edeka\", \"Aldi\", \"Konsum\"],\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfen ob auch alle keys in clean_places vorkommen\n",
    "for key, value in my_dict.items():\n",
    "    clean_places[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def calculate_similarity(string1, string2):\n",
    "    similarity = difflib.SequenceMatcher(None, string1.lower(), string2.lower()).ratio()\n",
    "    return similarity\n",
    "\n",
    "def map_similarity(string1, string2, threshold=0.8):\n",
    "    similarity = calculate_similarity(string1, string2)\n",
    "    if similarity >= threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Beispielaufruf\n",
    "string1 = \"Karl Heine Straße\"\n",
    "string2 = \"Karl-Heine-Straße\"\n",
    "threshold = 0.8\n",
    "result = map_similarity(string1, string2, threshold)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    # Umrechnung in Bogenmaß\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Deltas der Koordinaten\n",
    "    delta_lat = lat2_rad - lat1_rad\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Haversine-Formel\n",
    "    a = math.sin(delta_lat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    \n",
    "    # Erdradius in Metern\n",
    "    radius = 6371000\n",
    "    \n",
    "    # Berechnung der Distanz\n",
    "    distance = radius * c\n",
    "\n",
    "    return distance, 1/distance if distance > 0 else 0.0\n",
    "\n",
    "# Beispielaufruf\n",
    "lat1 = 51.338904\n",
    "lon1 = 12.323058\n",
    "lat2 = 51.340012\n",
    "lon2 = 12.334702\n",
    "\n",
    "result = distance(lat1, lon1, lat2, lon2)\n",
    "print(\"Die Distanz zwischen den Koordinaten beträgt:\", result, \"Meter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_data = []\n",
    "unknown_places = []\n",
    "for idx, row in df.iterrows():\n",
    "    places: List = row[\"places\"]\n",
    "    if row[\"latitude\"] > 52 or row[\"latitude\"] < 50:\n",
    "      continue\n",
    "    for place in places:\n",
    "      if place in clean_places.keys():\n",
    "        d, rd = distance(row[\"latitude\"], row[\"longitude\"], clean_places[place][\"lat\"], clean_places[place][\"lon\"])\n",
    "        arc_data.append({\n",
    "                \"name\": place,\n",
    "                \"from_lat\": row[\"latitude\"],\n",
    "                \"from_lon\": row[\"longitude\"],\n",
    "                \"to_lat\": clean_places[place][\"lat\"],\n",
    "                \"to_lon\": clean_places[place][\"lon\"],\n",
    "                \"free_from\": row[\"free_from\"],\n",
    "                \"distance\": d,\n",
    "                \"reverse_distance\": rd\n",
    "        })\n",
    "      else:\n",
    "        for key, value in my_dict.items():\n",
    "          if place in value:\n",
    "            d, rd = distance(row[\"latitude\"], row[\"longitude\"], clean_places[key][\"lat\"], clean_places[key][\"lon\"])\n",
    "            arc_data.append({\n",
    "                \"name\": key,\n",
    "                \"from_lat\": row[\"latitude\"],\n",
    "                \"from_lon\": row[\"longitude\"],\n",
    "                \"to_lat\": clean_places[key][\"lat\"],\n",
    "                \"to_lon\": clean_places[key][\"lon\"],\n",
    "                \"free_from\": row[\"free_from\"], \n",
    "                \"distance\": d,\n",
    "                \"reverse_distance\": rd\n",
    "            })\n",
    "          elif any([map_similarity(place, v) for v in value]):\n",
    "            d, rd = distance(row[\"latitude\"], row[\"longitude\"], clean_places[key][\"lat\"], clean_places[key][\"lon\"])\n",
    "            arc_data.append({\n",
    "                \"name\": key,\n",
    "                \"from_lat\": row[\"latitude\"],\n",
    "                \"from_lon\": row[\"longitude\"],\n",
    "                \"to_lat\": clean_places[key][\"lat\"],\n",
    "                \"to_lon\": clean_places[key][\"lon\"],\n",
    "                \"free_from\": row[\"free_from\"],\n",
    "                \"distance\": d,\n",
    "                \"reverse_distance\": rd\n",
    "            })\n",
    "          else:\n",
    "            unknown_places.append(place)\n",
    "unknown_places = set(unknown_places)\n",
    "print(len(unknown_places), unknown_places)\n",
    "arc_df = pd.DataFrame(data=arc_data)\n",
    "arc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Sprache analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gender_usage(text: str) -> Tuple[int, List[str], int, List[str], int, List[str]]:\n",
    "    # Apply spaCy NLP pipeline to the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract male pronouns\n",
    "    male_pronouns = [\n",
    "        token.text for token in doc\n",
    "        if token.pos_ == \"PRON\"\n",
    "        and token.text.lower() in [\"er\", \"ihn\", \"ihm\", \"sein\", \"seiner\", \"seinen\", \"seines\", \"dessen\", \"derjenige\", \"derjenige, der\", \"der\", \"seinerseits\"]\n",
    "   ]\n",
    "\n",
    "    # Extract female pronouns\n",
    "    female_pronouns = [\n",
    "        token.text for token in doc\n",
    "        if token.pos_ == \"PRON\"\n",
    "        and token.text.lower() in [\"sie\", \"ihr\", \"sie\", \"ihre\", \"ihrer\", \"ihren\", \"ihres\", \"deren\", \"diejenige\", \"diejenige, die\", \"die\", \"ihrerseits\"]\n",
    "    ]\n",
    "\n",
    "    # Extract neutral pronouns and gender-neutral terms   token.text.lower().endswith(\"ees\") \"Menschis\"\n",
    "    neutral_pronouns = [\n",
    "        token.text for token in doc\n",
    "        if token.text.lower() in [\"studierende\", \"teilnehmende\", \"lernende\", \"lehrende\", \"prüfende\", \"mitarbeitende\", \"beschäftigte\", \"studieninteressierte\", \"forschende\", \"absolventinnen und absolventen\", \"referierende\", \"promovierende\", \"suchende\", \"interessierte\", \"anfragende\", \"mieterinnen und mieter\", \"vermietende\", \"vertragspartner\", \"wohnungssuchende\", \"besichtigende\", \"maklerinnen und makler\", \"wohnungsanbieter\", \"umziehende\", \"wohnraumsuchende\"]\n",
    "    ]\n",
    "\n",
    "    # Return the counts and lists of pronouns\n",
    "    return len(male_pronouns), male_pronouns, len(female_pronouns), female_pronouns, len(neutral_pronouns), neutral_pronouns\n",
    "\n",
    "df[[\"male_pronouns\", \"male_pronouns_\",\n",
    "    \"female_pronouns\", \"female_pronouns_\",\n",
    "    \"neutral_pronouns\", \"neutral_pronouns_\"]\n",
    "   ] = df[\"full_description\"].apply(extract_gender_usage).apply(pd.Series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_count_coordinates(row:pd.Series, feature:str, idx:int) -> List[dict]:\n",
    "    tmp_list = []\n",
    "    for _ in range(row[feature]):\n",
    "        tmp_list.append({\n",
    "            \"index\": idx,\n",
    "            \"pronouns\": feature,\n",
    "            \"lat\": row[\"latitude\"],\n",
    "            \"lon\": row[\"longitude\"],\n",
    "            \"free_from\": row[\"free_from\"]\n",
    "            })\n",
    "    return tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row[\"latitude\"] > 55 or row[\"latitude\"] < 47:\n",
    "        continue\n",
    "    for feature in [\"female_pronouns\", \"male_pronouns\", \"neutral_pronouns\"]:\n",
    "        row_data = add_feature_count_coordinates(row, feature, idx)\n",
    "        if row_data:\n",
    "            gender_data.extend(row_data)\n",
    "\n",
    "gender_df = pd.DataFrame(data=gender_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[82,48,140,150,148], [30, 32, 34]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering tf/idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vektorisieren der Textdaten\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"full_description\"])\n",
    "# Anwenden des K-Means-Clustering-Algorithmus\n",
    "num_clusters = 4  # Anzahl der gewünschten Cluster\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=2023)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Hinzufügen der Clusterlabels zum DataFrame\n",
    "df[\"cluster_label\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{data_path}wg_gesucht.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_df.to_csv(f\"{data_path}gender_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_df.to_csv(f\"{data_path}arc_wg_gesucht.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing json\n",
    "json_object = json.dumps(places_dict, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(f\"{data_path}places.json\", \"w\", encoding='utf-8') as outfile:\n",
    "    outfile.write(json_object)\n",
    "    \n",
    "df[['full_description']].to_excel(f\"{data_path}description.xlsx\", index=False)\n",
    "df.to_excel(f\"{data_path}wg_gesucht.xlsx\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
